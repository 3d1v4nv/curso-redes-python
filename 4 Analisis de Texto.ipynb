{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el análisis del contenido de los tweets podemos obtener información adicional, por ejemplo, los temas que se está tratando, si se comparten páginas web, etc. Existen módulos y herramientas especializadas para realizar análisis de texto, sin embargo en este curso sólo utilizaremos las herramientas básicas de python para comparar cadenas de caracteres.\n",
    "\n",
    "Si hemos seguido el notebook hasta este punto, debemos tener en la carpeta _data_ el archivo ¨stream_lunch.json¨, el cual contiene un conjunto de tweets obtenidos a traves del stremming. Uno de los procesos que facilita análisi texto es sustituir los caracteres con acento y signos de interrogación y admiración de apertura (\"¿\",\"¡\") por caracteres sin acento y sólo signos de cierre \"?\" y \"!\". Además, podemos remover ligas a páginas web para que las palabras en ellas no interfieran en el análisis y reemplazar mayúsculas a minúsculas. Esto lo hacemos porque no todos los usuarios incluyen acentos, signos iniciales \"¿\",\"¡\", páginas o mayúsculas.\n",
    "\n",
    "Comencemos extrayendo el texto de todos los tweets, sustituyendo caractéres y guardándolo en un archivo independiente para facilitar una posible inspección visual. El siguiente código extraemos tweets con un cierto tema y ademas guarda en memoria una lista `Textos` con el contenido de cada tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "import json\n",
    "import tweepy\n",
    "\n",
    "consumer_key='XXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "consumer_secret= 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "authentication = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "authentication.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(authentication)\n",
    "\n",
    "with open(\"./data/stream_lunch.json\",\"w\") as archivo:\n",
    "    pass\n",
    "class TwitterListener(StreamListener):\n",
    "\n",
    "    def on_data(self,data):\n",
    "        tweet=json.loads(str(data))\n",
    "        print(tweet['text'])\n",
    "        with open(\"./data/stream_lunch.json\",\"a\") as archivo:\n",
    "            archivo.write(json.dumps(tweet))\n",
    "            archivo.write('\\n')\n",
    "        return(True)\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "while True:\n",
    "        try:\n",
    "            twitter_stream = Stream(authentication, TwitterListener())\n",
    "            twitter_stream.filter(track=['#FelizViernes'])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora leemos los tweets almacenado en el archivo `stream_lunch.json` y guardamos los textos en una lista llamada `Textos`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from unidecode import unidecode\n",
    "\n",
    "with open(\"./data/stream_lunch.json\",\"r\") as archivo:\n",
    "    renglones=archivo.readlines()\n",
    "\n",
    "Textos=[]\n",
    "with open(\"./data/Textos.txt\",\"w\") as archivo2:\n",
    "    for data in renglones:\n",
    "        tuit=json.loads(data)\n",
    "        texto=unidecode(tuit['text']) #\n",
    "        texto.encode('ascii')# Reemplazan caracteres por su equivalente ascii\n",
    "        Textos.append(texto)\n",
    "        archivo2.write(texto)\n",
    "        archivo2.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el efecto de sustituir los caracteres acentuados imprimiendo el texto original y el modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L=[json.loads(data)['text'] for data in renglones]\n",
    "for i in range(13):\n",
    "    print(L[i])\n",
    "    print(Textos[i])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos remover los links que se comparten en el texto, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(10):\n",
    "    sin_url = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', Textos[i])\n",
    "    print(sin_url)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repitamos el proceso de remover direcciones de páginas para todos los textos y los almacenémoslos de esta manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "Textos=[re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', texto) for texto in Textos]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(Textos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, resulta conveniente reemplazar todas las letras mayúsculas por minúsculas, pues recordemos que Python las interpreta como caracteres distintos, esto lo podemos hcer mediante el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Textos=[texto.lower() for texto in Textos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El contenido del texto lo analizaremos a partir de las palabras que aparecen en él, sin embargo no todas las palabras tienen la misma importancia, por ejemplo, los artículos, pronombres, preposiciones, etc no son palabras representativas del contenido, y por esta razón a estas palabras comúnmente se les denomina _palabras vacías_ ó _stop words_. En la carpeta _data_ se encuentra una lista de _palabras vacías_ que puede extenderse y adaptarse. El siguiente código lee y almacena en una lista las _palabras stop_ para posteriormente utilizarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#palabras_vacias='./data/stop_words.txt'\n",
    "with open('./data/stopwordES.txt',\"r\") as archivo:\n",
    "    palabras_stop=archivo.readlines()\n",
    "    \n",
    "palabras_stop=[palabra[:-1] for palabra in palabras_stop]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(palabras_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de las palabras _vacías_ o _stop_, podemos crear listas adicionales con palabras asociadas a un tema; ya que si recolectamos los tweets buscando un _hashtag_ o una palabra en particular, tenemos una idea de qué palabras pueden estar presentes. Por ejemplo, si recolectamos tweets buscando la palabra \"accidente\", muchos de los tweets pueden contener además la palabra \"automovil\", \"derrumbe\", etc, y de igual manera evitar los tweets que contenga otras palabras relacionadas con otros temas. Por ejemplo, para ver cuántos de los tweets cuyo mensaje almacenamos en la variable ``Textos` contienen palabras asociadas a un tema, podemos usar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "semantica=['mañana','dia','amanecer']\n",
    "contador=0\n",
    "\n",
    "for texto in Textos:\n",
    "    for palabra in semantica:\n",
    "        if palabra in texto:\n",
    "             contador=contador+1\n",
    "print(contador)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Recuerda que buscamos palabras con letras mayúsculas nunca las vamos a encontrar, pues hemos sustitutido letras mayúsculas por minúsculas\n",
    "\n",
    "**Ejercicio:** Modifica las palabras a buscar de acuerdo a tus intereses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un análisis más fino requiere analizar palabra por palabra, así podríamos identificar las palabras de mayor frecuencia. Primero debemos dividir cada texto en palabras, y después almacenar las palabras que no sean _stop_ junto con el número de veces que es usada. Para agilizar este progeso, sólo compararemos palabras que no inicien con caracteres numéricos o caracteres _especiales_ (\"@\" , \"#\", \"$\",\"%\"). Veamos un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diccionario={}\n",
    "#Creamos un diccionario de las palabras que no son stop\n",
    "for texto in Textos:\n",
    "    t=texto.split(' ') # separa por cadena de texto por palabras y las guarda en una lista t\n",
    "    for palabra in t:\n",
    "        if palabra != '':\n",
    "            if (palabra not in palabras_stop) and (palabra[0] not in range(10)) and (palabra[0] not in [\"@\",\"#\",\"$\",\"%\",\"/\",\"\\n\",\"!\",\"?\"]) :\n",
    "                diccionario[palabra]=0\n",
    "\n",
    "            \n",
    "#Corremos un codigo similar pero ahora realiza el conteo de las apariciones de las palabras en el diccionario\n",
    "for texto in Textos:\n",
    "    t=texto.split(' ') \n",
    "    for palabra in t:\n",
    "        if palabra != '':\n",
    "            if (palabra not in palabras_stop) and (palabra[0] not in range(10)) and (palabra[0] not in [\"@\",\"#\",\"$\",\"%\",\"/\",\"\\n\",\"!\",\"?\"]) :\n",
    "                diccionario[palabra]=diccionario[palabra]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionemos el diccionario buscando los valores de ocurrencia más altas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valores=[diccionario[valor] for valor in diccionario.keys()] # Lista los valores de cada palabra en el diccionario\n",
    "valores.sort()  #Ordena la lista de menor a mayor \n",
    "valores.reverse() #invierte el orden de la lista \n",
    "\n",
    "print(valores[:10])  #imprime los 10 valores más altos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que sabemos el rango de ocurrencia de las palabras, busquemos en el diccionario aquellas palabras con un índice alto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alta_ocurr=[]\n",
    "\n",
    "for palabra in diccionario:\n",
    "    if diccionario[palabra]>= : #Debemos incluir un valor numerico por el valor en la frecuencia que nos interese encontrar\n",
    "        alta_ocurr.append(palabra)\n",
    "        \n",
    "print(alta_ocurr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos interesan saber cuales son todas las palabras que encontramos solo debemos escribir lo siguiente:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(diccionario.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
